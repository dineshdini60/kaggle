{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Template\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Importing the dataset\n",
    "datatrain = pd.read_csv('train.csv')\n",
    "datatest = pd.read_csv('test.csv')\n",
    "y_train = datatrain.iloc[:,[1]].values\n",
    "y = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datatrain.iloc[:,[2,4,5,6,7,9]].values\n",
    "X_test_scaled = datatest.iloc[:,[1,3,4,5,6,8]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatest.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values= 'NaN', strategy=\"mean\",axis= 0)\n",
    "X[:,[2]] = imp.fit_transform(X[:,[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values= 'NaN', strategy=\"mean\",axis= 0)\n",
    "X_test_scaled[:,[2]] = imp.fit_transform(X_test_scaled[:,[2]])\n",
    "X_test_scaled[:,[-1]] = imp.fit_transform(X_test_scaled[:,[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X.fit_transform(X[:, 1])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datatrain.iloc[:,[2,4,5,6,7,9]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X_test_scaled[:, 1] = labelencoder_X.fit_transform(X_test_scaled[:, 1])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X_test_scaled = onehotencoder.fit_transform(X_test_scaled).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,1:]\n",
    "X_test_scaled = X_test_scaled[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X)\n",
    "#X_test_scaled = sc.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scr = StandardScaler()\n",
    "X_test_scaled = sc.fit_transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73769513,  0.82737724, -0.5924806 ,  0.43279337, -0.47367361,\n",
       "       -0.50244517])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75592895,  0.87348191,  0.3349926 , -0.49947002, -0.4002477 ,\n",
       "       -0.49840706])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_lr = LogisticRegression(random_state = 0)\n",
    "classifier_lr.fit(X_train_scaled, y)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_lr = classifier_lr.predict((X_test_scaled))\n",
    "out = y_pred_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier_knn.fit(X_train_scaled, y)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_knn = classifier_knn.predict(X_test_scaled)\n",
    "\n",
    "out = out+y_pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier_ksvm = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_ksvm.fit(X_train_scaled, y)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_ksvm = classifier_ksvm.predict(X_test_scaled)\n",
    "\n",
    "out = out+y_pred_ksvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_rf = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)\n",
    "classifier_rf.fit(X_train_scaled, y)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_rf = classifier_rf.predict(X_test_scaled)\n",
    "out = out+y_pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_nb = GaussianNB()\n",
    "classifier_nb.fit(X_train_scaled, y)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_nb = classifier_nb.predict(X_test_scaled)\n",
    "\n",
    "out = out+y_pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = list()\n",
    "out = out/5\n",
    "for i in out:\n",
    "    if(i>0.5):\n",
    "        y_final.append(1)\n",
    "    else:\n",
    "        y_final.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = datatest['PassengerId'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_avg = np.column_stack((pid,y_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 892,    0],\n",
       "       [ 893,    0],\n",
       "       [ 894,    0],\n",
       "       [ 895,    0],\n",
       "       [ 896,    1],\n",
       "       [ 897,    0],\n",
       "       [ 898,    1],\n",
       "       [ 899,    0],\n",
       "       [ 900,    1],\n",
       "       [ 901,    0],\n",
       "       [ 902,    0],\n",
       "       [ 903,    0],\n",
       "       [ 904,    1],\n",
       "       [ 905,    0],\n",
       "       [ 906,    1],\n",
       "       [ 907,    1],\n",
       "       [ 908,    0],\n",
       "       [ 909,    0],\n",
       "       [ 910,    1],\n",
       "       [ 911,    0],\n",
       "       [ 912,    0],\n",
       "       [ 913,    0],\n",
       "       [ 914,    1],\n",
       "       [ 915,    0],\n",
       "       [ 916,    1],\n",
       "       [ 917,    0],\n",
       "       [ 918,    1],\n",
       "       [ 919,    0],\n",
       "       [ 920,    0],\n",
       "       [ 921,    0],\n",
       "       [ 922,    0],\n",
       "       [ 923,    0],\n",
       "       [ 924,    0],\n",
       "       [ 925,    0],\n",
       "       [ 926,    0],\n",
       "       [ 927,    0],\n",
       "       [ 928,    1],\n",
       "       [ 929,    1],\n",
       "       [ 930,    0],\n",
       "       [ 931,    0],\n",
       "       [ 932,    0],\n",
       "       [ 933,    0],\n",
       "       [ 934,    0],\n",
       "       [ 935,    1],\n",
       "       [ 936,    1],\n",
       "       [ 937,    0],\n",
       "       [ 938,    0],\n",
       "       [ 939,    0],\n",
       "       [ 940,    1],\n",
       "       [ 941,    1],\n",
       "       [ 942,    0],\n",
       "       [ 943,    0],\n",
       "       [ 944,    1],\n",
       "       [ 945,    1],\n",
       "       [ 946,    0],\n",
       "       [ 947,    0],\n",
       "       [ 948,    0],\n",
       "       [ 949,    0],\n",
       "       [ 950,    0],\n",
       "       [ 951,    1],\n",
       "       [ 952,    0],\n",
       "       [ 953,    0],\n",
       "       [ 954,    0],\n",
       "       [ 955,    1],\n",
       "       [ 956,    1],\n",
       "       [ 957,    1],\n",
       "       [ 958,    1],\n",
       "       [ 959,    0],\n",
       "       [ 960,    0],\n",
       "       [ 961,    1],\n",
       "       [ 962,    1],\n",
       "       [ 963,    0],\n",
       "       [ 964,    1],\n",
       "       [ 965,    0],\n",
       "       [ 966,    1],\n",
       "       [ 967,    0],\n",
       "       [ 968,    0],\n",
       "       [ 969,    1],\n",
       "       [ 970,    0],\n",
       "       [ 971,    1],\n",
       "       [ 972,    1],\n",
       "       [ 973,    0],\n",
       "       [ 974,    0],\n",
       "       [ 975,    0],\n",
       "       [ 976,    0],\n",
       "       [ 977,    0],\n",
       "       [ 978,    1],\n",
       "       [ 979,    1],\n",
       "       [ 980,    1],\n",
       "       [ 981,    1],\n",
       "       [ 982,    1],\n",
       "       [ 983,    0],\n",
       "       [ 984,    1],\n",
       "       [ 985,    0],\n",
       "       [ 986,    0],\n",
       "       [ 987,    0],\n",
       "       [ 988,    1],\n",
       "       [ 989,    0],\n",
       "       [ 990,    1],\n",
       "       [ 991,    0],\n",
       "       [ 992,    1],\n",
       "       [ 993,    0],\n",
       "       [ 994,    0],\n",
       "       [ 995,    0],\n",
       "       [ 996,    1],\n",
       "       [ 997,    0],\n",
       "       [ 998,    0],\n",
       "       [ 999,    0],\n",
       "       [1000,    0],\n",
       "       [1001,    0],\n",
       "       [1002,    0],\n",
       "       [1003,    1],\n",
       "       [1004,    1],\n",
       "       [1005,    1],\n",
       "       [1006,    1],\n",
       "       [1007,    0],\n",
       "       [1008,    0],\n",
       "       [1009,    1],\n",
       "       [1010,    0],\n",
       "       [1011,    1],\n",
       "       [1012,    1],\n",
       "       [1013,    0],\n",
       "       [1014,    1],\n",
       "       [1015,    0],\n",
       "       [1016,    0],\n",
       "       [1017,    1],\n",
       "       [1018,    0],\n",
       "       [1019,    0],\n",
       "       [1020,    0],\n",
       "       [1021,    0],\n",
       "       [1022,    0],\n",
       "       [1023,    0],\n",
       "       [1024,    1],\n",
       "       [1025,    0],\n",
       "       [1026,    0],\n",
       "       [1027,    0],\n",
       "       [1028,    0],\n",
       "       [1029,    0],\n",
       "       [1030,    1],\n",
       "       [1031,    0],\n",
       "       [1032,    0],\n",
       "       [1033,    1],\n",
       "       [1034,    0],\n",
       "       [1035,    0],\n",
       "       [1036,    0],\n",
       "       [1037,    0],\n",
       "       [1038,    0],\n",
       "       [1039,    0],\n",
       "       [1040,    0],\n",
       "       [1041,    0],\n",
       "       [1042,    1],\n",
       "       [1043,    0],\n",
       "       [1044,    0],\n",
       "       [1045,    1],\n",
       "       [1046,    0],\n",
       "       [1047,    0],\n",
       "       [1048,    1],\n",
       "       [1049,    1],\n",
       "       [1050,    0],\n",
       "       [1051,    1],\n",
       "       [1052,    1],\n",
       "       [1053,    1],\n",
       "       [1054,    1],\n",
       "       [1055,    0],\n",
       "       [1056,    0],\n",
       "       [1057,    1],\n",
       "       [1058,    0],\n",
       "       [1059,    0],\n",
       "       [1060,    1],\n",
       "       [1061,    1],\n",
       "       [1062,    0],\n",
       "       [1063,    0],\n",
       "       [1064,    0],\n",
       "       [1065,    0],\n",
       "       [1066,    0],\n",
       "       [1067,    1],\n",
       "       [1068,    1],\n",
       "       [1069,    0],\n",
       "       [1070,    1],\n",
       "       [1071,    1],\n",
       "       [1072,    0],\n",
       "       [1073,    0],\n",
       "       [1074,    1],\n",
       "       [1075,    0],\n",
       "       [1076,    1],\n",
       "       [1077,    0],\n",
       "       [1078,    1],\n",
       "       [1079,    0],\n",
       "       [1080,    0],\n",
       "       [1081,    0],\n",
       "       [1082,    0],\n",
       "       [1083,    0],\n",
       "       [1084,    0],\n",
       "       [1085,    0],\n",
       "       [1086,    1],\n",
       "       [1087,    0],\n",
       "       [1088,    1],\n",
       "       [1089,    1],\n",
       "       [1090,    0],\n",
       "       [1091,    1],\n",
       "       [1092,    1],\n",
       "       [1093,    1],\n",
       "       [1094,    0],\n",
       "       [1095,    1],\n",
       "       [1096,    0],\n",
       "       [1097,    0],\n",
       "       [1098,    1],\n",
       "       [1099,    0],\n",
       "       [1100,    1],\n",
       "       [1101,    0],\n",
       "       [1102,    0],\n",
       "       [1103,    0],\n",
       "       [1104,    0],\n",
       "       [1105,    1],\n",
       "       [1106,    0],\n",
       "       [1107,    0],\n",
       "       [1108,    1],\n",
       "       [1109,    0],\n",
       "       [1110,    1],\n",
       "       [1111,    0],\n",
       "       [1112,    1],\n",
       "       [1113,    0],\n",
       "       [1114,    1],\n",
       "       [1115,    0],\n",
       "       [1116,    1],\n",
       "       [1117,    1],\n",
       "       [1118,    0],\n",
       "       [1119,    1],\n",
       "       [1120,    0],\n",
       "       [1121,    0],\n",
       "       [1122,    0],\n",
       "       [1123,    1],\n",
       "       [1124,    0],\n",
       "       [1125,    0],\n",
       "       [1126,    0],\n",
       "       [1127,    0],\n",
       "       [1128,    0],\n",
       "       [1129,    0],\n",
       "       [1130,    1],\n",
       "       [1131,    1],\n",
       "       [1132,    1],\n",
       "       [1133,    1],\n",
       "       [1134,    0],\n",
       "       [1135,    0],\n",
       "       [1136,    0],\n",
       "       [1137,    0],\n",
       "       [1138,    1],\n",
       "       [1139,    0],\n",
       "       [1140,    1],\n",
       "       [1141,    1],\n",
       "       [1142,    1],\n",
       "       [1143,    0],\n",
       "       [1144,    0],\n",
       "       [1145,    0],\n",
       "       [1146,    0],\n",
       "       [1147,    0],\n",
       "       [1148,    0],\n",
       "       [1149,    0],\n",
       "       [1150,    1],\n",
       "       [1151,    0],\n",
       "       [1152,    0],\n",
       "       [1153,    0],\n",
       "       [1154,    1],\n",
       "       [1155,    1],\n",
       "       [1156,    0],\n",
       "       [1157,    0],\n",
       "       [1158,    0],\n",
       "       [1159,    0],\n",
       "       [1160,    1],\n",
       "       [1161,    0],\n",
       "       [1162,    0],\n",
       "       [1163,    0],\n",
       "       [1164,    1],\n",
       "       [1165,    1],\n",
       "       [1166,    0],\n",
       "       [1167,    1],\n",
       "       [1168,    0],\n",
       "       [1169,    0],\n",
       "       [1170,    0],\n",
       "       [1171,    0],\n",
       "       [1172,    1],\n",
       "       [1173,    1],\n",
       "       [1174,    1],\n",
       "       [1175,    1],\n",
       "       [1176,    1],\n",
       "       [1177,    0],\n",
       "       [1178,    0],\n",
       "       [1179,    0],\n",
       "       [1180,    0],\n",
       "       [1181,    0],\n",
       "       [1182,    0],\n",
       "       [1183,    1],\n",
       "       [1184,    0],\n",
       "       [1185,    0],\n",
       "       [1186,    0],\n",
       "       [1187,    0],\n",
       "       [1188,    1],\n",
       "       [1189,    0],\n",
       "       [1190,    0],\n",
       "       [1191,    0],\n",
       "       [1192,    0],\n",
       "       [1193,    0],\n",
       "       [1194,    0],\n",
       "       [1195,    0],\n",
       "       [1196,    1],\n",
       "       [1197,    1],\n",
       "       [1198,    1],\n",
       "       [1199,    1],\n",
       "       [1200,    0],\n",
       "       [1201,    0],\n",
       "       [1202,    0],\n",
       "       [1203,    0],\n",
       "       [1204,    0],\n",
       "       [1205,    1],\n",
       "       [1206,    1],\n",
       "       [1207,    1],\n",
       "       [1208,    0],\n",
       "       [1209,    0],\n",
       "       [1210,    0],\n",
       "       [1211,    0],\n",
       "       [1212,    0],\n",
       "       [1213,    0],\n",
       "       [1214,    0],\n",
       "       [1215,    0],\n",
       "       [1216,    1],\n",
       "       [1217,    0],\n",
       "       [1218,    1],\n",
       "       [1219,    0],\n",
       "       [1220,    0],\n",
       "       [1221,    0],\n",
       "       [1222,    1],\n",
       "       [1223,    0],\n",
       "       [1224,    0],\n",
       "       [1225,    1],\n",
       "       [1226,    0],\n",
       "       [1227,    0],\n",
       "       [1228,    0],\n",
       "       [1229,    0],\n",
       "       [1230,    0],\n",
       "       [1231,    0],\n",
       "       [1232,    0],\n",
       "       [1233,    0],\n",
       "       [1234,    0],\n",
       "       [1235,    1],\n",
       "       [1236,    0],\n",
       "       [1237,    1],\n",
       "       [1238,    0],\n",
       "       [1239,    1],\n",
       "       [1240,    0],\n",
       "       [1241,    1],\n",
       "       [1242,    1],\n",
       "       [1243,    0],\n",
       "       [1244,    0],\n",
       "       [1245,    0],\n",
       "       [1246,    1],\n",
       "       [1247,    0],\n",
       "       [1248,    1],\n",
       "       [1249,    0],\n",
       "       [1250,    0],\n",
       "       [1251,    1],\n",
       "       [1252,    0],\n",
       "       [1253,    1],\n",
       "       [1254,    1],\n",
       "       [1255,    0],\n",
       "       [1256,    1],\n",
       "       [1257,    0],\n",
       "       [1258,    0],\n",
       "       [1259,    1],\n",
       "       [1260,    1],\n",
       "       [1261,    0],\n",
       "       [1262,    0],\n",
       "       [1263,    1],\n",
       "       [1264,    0],\n",
       "       [1265,    0],\n",
       "       [1266,    1],\n",
       "       [1267,    1],\n",
       "       [1268,    0],\n",
       "       [1269,    0],\n",
       "       [1270,    0],\n",
       "       [1271,    0],\n",
       "       [1272,    0],\n",
       "       [1273,    0],\n",
       "       [1274,    1],\n",
       "       [1275,    1],\n",
       "       [1276,    0],\n",
       "       [1277,    1],\n",
       "       [1278,    0],\n",
       "       [1279,    0],\n",
       "       [1280,    0],\n",
       "       [1281,    0],\n",
       "       [1282,    0],\n",
       "       [1283,    1],\n",
       "       [1284,    1],\n",
       "       [1285,    0],\n",
       "       [1286,    0],\n",
       "       [1287,    1],\n",
       "       [1288,    0],\n",
       "       [1289,    1],\n",
       "       [1290,    0],\n",
       "       [1291,    0],\n",
       "       [1292,    1],\n",
       "       [1293,    0],\n",
       "       [1294,    1],\n",
       "       [1295,    0],\n",
       "       [1296,    0],\n",
       "       [1297,    0],\n",
       "       [1298,    0],\n",
       "       [1299,    0],\n",
       "       [1300,    1],\n",
       "       [1301,    1],\n",
       "       [1302,    1],\n",
       "       [1303,    1],\n",
       "       [1304,    1],\n",
       "       [1305,    0],\n",
       "       [1306,    1],\n",
       "       [1307,    0],\n",
       "       [1308,    0],\n",
       "       [1309,    0]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chall\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\chall\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=6, activation=\"relu\", kernel_initializer=\"uniform\", units=4)`\n",
      "  \"\"\"\n",
      "C:\\Users\\chall\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", kernel_initializer=\"uniform\", units=3)`\n",
      "  \n",
      "C:\\Users\\chall\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", kernel_initializer=\"uniform\", units=1)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "891/891 [==============================] - 0s 318us/step - loss: 0.6891 - acc: 0.6049\n",
      "Epoch 2/200\n",
      "891/891 [==============================] - 0s 123us/step - loss: 0.6724 - acc: 0.6162\n",
      "Epoch 3/200\n",
      "891/891 [==============================] - 0s 120us/step - loss: 0.6323 - acc: 0.6162\n",
      "Epoch 4/200\n",
      "891/891 [==============================] - 0s 114us/step - loss: 0.5875 - acc: 0.6162\n",
      "Epoch 5/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.5614 - acc: 0.6162\n",
      "Epoch 6/200\n",
      "891/891 [==============================] - 0s 115us/step - loss: 0.5482 - acc: 0.6162\n",
      "Epoch 7/200\n",
      "891/891 [==============================] - 0s 113us/step - loss: 0.5391 - acc: 0.6162\n",
      "Epoch 8/200\n",
      "891/891 [==============================] - 0s 122us/step - loss: 0.5322 - acc: 0.7710\n",
      "Epoch 9/200\n",
      "891/891 [==============================] - 0s 107us/step - loss: 0.5265 - acc: 0.7946\n",
      "Epoch 10/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.5210 - acc: 0.7957\n",
      "Epoch 11/200\n",
      "891/891 [==============================] - 0s 116us/step - loss: 0.5169 - acc: 0.8103\n",
      "Epoch 12/200\n",
      "891/891 [==============================] - 0s 117us/step - loss: 0.5117 - acc: 0.8058\n",
      "Epoch 13/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.5077 - acc: 0.8070\n",
      "Epoch 14/200\n",
      "891/891 [==============================] - 0s 110us/step - loss: 0.5043 - acc: 0.8081\n",
      "Epoch 15/200\n",
      "891/891 [==============================] - 0s 118us/step - loss: 0.5005 - acc: 0.8103\n",
      "Epoch 16/200\n",
      "891/891 [==============================] - 0s 132us/step - loss: 0.4970 - acc: 0.8103\n",
      "Epoch 17/200\n",
      "891/891 [==============================] - 0s 133us/step - loss: 0.4945 - acc: 0.8103\n",
      "Epoch 18/200\n",
      "891/891 [==============================] - 0s 134us/step - loss: 0.4915 - acc: 0.8070\n",
      "Epoch 19/200\n",
      "891/891 [==============================] - 0s 108us/step - loss: 0.4893 - acc: 0.8081\n",
      "Epoch 20/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4867 - acc: 0.8070\n",
      "Epoch 21/200\n",
      "891/891 [==============================] - 0s 135us/step - loss: 0.4845 - acc: 0.8081\n",
      "Epoch 22/200\n",
      "891/891 [==============================] - 0s 138us/step - loss: 0.4826 - acc: 0.8047\n",
      "Epoch 23/200\n",
      "891/891 [==============================] - 0s 136us/step - loss: 0.4806 - acc: 0.8036\n",
      "Epoch 24/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.4788 - acc: 0.8070\n",
      "Epoch 25/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4768 - acc: 0.8047\n",
      "Epoch 26/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.4753 - acc: 0.8058\n",
      "Epoch 27/200\n",
      "891/891 [==============================] - 0s 115us/step - loss: 0.4740 - acc: 0.8058\n",
      "Epoch 28/200\n",
      "891/891 [==============================] - 0s 120us/step - loss: 0.4729 - acc: 0.8092\n",
      "Epoch 29/200\n",
      "891/891 [==============================] - 0s 135us/step - loss: 0.4714 - acc: 0.8114\n",
      "Epoch 30/200\n",
      "891/891 [==============================] - 0s 146us/step - loss: 0.4696 - acc: 0.8103\n",
      "Epoch 31/200\n",
      "891/891 [==============================] - 0s 141us/step - loss: 0.4684 - acc: 0.8092\n",
      "Epoch 32/200\n",
      "891/891 [==============================] - 0s 115us/step - loss: 0.4677 - acc: 0.8126\n",
      "Epoch 33/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4662 - acc: 0.8126\n",
      "Epoch 34/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4654 - acc: 0.8126\n",
      "Epoch 35/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4641 - acc: 0.8126\n",
      "Epoch 36/200\n",
      "891/891 [==============================] - 0s 107us/step - loss: 0.4631 - acc: 0.8148\n",
      "Epoch 37/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4620 - acc: 0.8137\n",
      "Epoch 38/200\n",
      "891/891 [==============================] - 0s 104us/step - loss: 0.4608 - acc: 0.8148\n",
      "Epoch 39/200\n",
      "891/891 [==============================] - 0s 106us/step - loss: 0.4600 - acc: 0.8148\n",
      "Epoch 40/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4585 - acc: 0.8171\n",
      "Epoch 41/200\n",
      "891/891 [==============================] - 0s 122us/step - loss: 0.4575 - acc: 0.8182\n",
      "Epoch 42/200\n",
      "891/891 [==============================] - 0s 131us/step - loss: 0.4564 - acc: 0.8171\n",
      "Epoch 43/200\n",
      "891/891 [==============================] - 0s 126us/step - loss: 0.4553 - acc: 0.8171\n",
      "Epoch 44/200\n",
      "891/891 [==============================] - 0s 123us/step - loss: 0.4545 - acc: 0.8182\n",
      "Epoch 45/200\n",
      "891/891 [==============================] - 0s 124us/step - loss: 0.4533 - acc: 0.8182\n",
      "Epoch 46/200\n",
      "891/891 [==============================] - 0s 124us/step - loss: 0.4526 - acc: 0.8182\n",
      "Epoch 47/200\n",
      "891/891 [==============================] - 0s 110us/step - loss: 0.4510 - acc: 0.8182\n",
      "Epoch 48/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4503 - acc: 0.8182\n",
      "Epoch 49/200\n",
      "891/891 [==============================] - 0s 108us/step - loss: 0.4491 - acc: 0.8193\n",
      "Epoch 50/200\n",
      "891/891 [==============================] - 0s 128us/step - loss: 0.4480 - acc: 0.8182\n",
      "Epoch 51/200\n",
      "891/891 [==============================] - 0s 122us/step - loss: 0.4475 - acc: 0.8171\n",
      "Epoch 52/200\n",
      "891/891 [==============================] - 0s 120us/step - loss: 0.4463 - acc: 0.8193\n",
      "Epoch 53/200\n",
      "891/891 [==============================] - 0s 107us/step - loss: 0.4452 - acc: 0.8227\n",
      "Epoch 54/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4443 - acc: 0.8227\n",
      "Epoch 55/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4430 - acc: 0.8238\n",
      "Epoch 56/200\n",
      "891/891 [==============================] - 0s 107us/step - loss: 0.4423 - acc: 0.8249\n",
      "Epoch 57/200\n",
      "891/891 [==============================] - 0s 107us/step - loss: 0.4416 - acc: 0.8283\n",
      "Epoch 58/200\n",
      "891/891 [==============================] - 0s 126us/step - loss: 0.4409 - acc: 0.8249\n",
      "Epoch 59/200\n",
      "891/891 [==============================] - 0s 128us/step - loss: 0.4399 - acc: 0.8227\n",
      "Epoch 60/200\n",
      "891/891 [==============================] - 0s 124us/step - loss: 0.4395 - acc: 0.8238\n",
      "Epoch 61/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.4385 - acc: 0.8249\n",
      "Epoch 62/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4376 - acc: 0.8238\n",
      "Epoch 63/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4367 - acc: 0.8238\n",
      "Epoch 64/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4363 - acc: 0.8204\n",
      "Epoch 65/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4354 - acc: 0.8215\n",
      "Epoch 66/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4350 - acc: 0.8215\n",
      "Epoch 67/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4341 - acc: 0.8227\n",
      "Epoch 68/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4337 - acc: 0.8238\n",
      "Epoch 69/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4330 - acc: 0.8249\n",
      "Epoch 70/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4328 - acc: 0.8238\n",
      "Epoch 71/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4317 - acc: 0.8249\n",
      "Epoch 72/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4314 - acc: 0.8249\n",
      "Epoch 73/200\n",
      "891/891 [==============================] - 0s 96us/step - loss: 0.4310 - acc: 0.8238\n",
      "Epoch 74/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4302 - acc: 0.8249\n",
      "Epoch 75/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4303 - acc: 0.8238\n",
      "Epoch 76/200\n",
      "891/891 [==============================] - 0s 98us/step - loss: 0.4301 - acc: 0.8260\n",
      "Epoch 77/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4288 - acc: 0.8238\n",
      "Epoch 78/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4283 - acc: 0.8249\n",
      "Epoch 79/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4281 - acc: 0.8249\n",
      "Epoch 80/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4278 - acc: 0.8260\n",
      "Epoch 81/200\n",
      "891/891 [==============================] - 0s 98us/step - loss: 0.4273 - acc: 0.8260\n",
      "Epoch 82/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4264 - acc: 0.8238\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891/891 [==============================] - 0s 100us/step - loss: 0.4262 - acc: 0.8272\n",
      "Epoch 84/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4262 - acc: 0.8260\n",
      "Epoch 85/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4256 - acc: 0.8193\n",
      "Epoch 86/200\n",
      "891/891 [==============================] - 0s 95us/step - loss: 0.4252 - acc: 0.8260\n",
      "Epoch 87/200\n",
      "891/891 [==============================] - 0s 106us/step - loss: 0.4248 - acc: 0.8238\n",
      "Epoch 88/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4241 - acc: 0.8272\n",
      "Epoch 89/200\n",
      "891/891 [==============================] - 0s 98us/step - loss: 0.4243 - acc: 0.8249\n",
      "Epoch 90/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4242 - acc: 0.8272\n",
      "Epoch 91/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4233 - acc: 0.8249\n",
      "Epoch 92/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4235 - acc: 0.8238\n",
      "Epoch 93/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4229 - acc: 0.8249\n",
      "Epoch 94/200\n",
      "891/891 [==============================] - 0s 103us/step - loss: 0.4225 - acc: 0.8260\n",
      "Epoch 95/200\n",
      "891/891 [==============================] - 0s 118us/step - loss: 0.4220 - acc: 0.8249\n",
      "Epoch 96/200\n",
      "891/891 [==============================] - 0s 143us/step - loss: 0.4219 - acc: 0.8260\n",
      "Epoch 97/200\n",
      "891/891 [==============================] - 0s 143us/step - loss: 0.4212 - acc: 0.8260\n",
      "Epoch 98/200\n",
      "891/891 [==============================] - 0s 144us/step - loss: 0.4210 - acc: 0.8272\n",
      "Epoch 99/200\n",
      "891/891 [==============================] - 0s 140us/step - loss: 0.4208 - acc: 0.8260\n",
      "Epoch 100/200\n",
      "891/891 [==============================] - 0s 108us/step - loss: 0.4204 - acc: 0.8260\n",
      "Epoch 101/200\n",
      "891/891 [==============================] - 0s 93us/step - loss: 0.4204 - acc: 0.8249\n",
      "Epoch 102/200\n",
      "891/891 [==============================] - 0s 124us/step - loss: 0.4204 - acc: 0.8227\n",
      "Epoch 103/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4197 - acc: 0.8260\n",
      "Epoch 104/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4194 - acc: 0.8227\n",
      "Epoch 105/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4197 - acc: 0.8260\n",
      "Epoch 106/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4194 - acc: 0.8260\n",
      "Epoch 107/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4188 - acc: 0.8249\n",
      "Epoch 108/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4186 - acc: 0.8238\n",
      "Epoch 109/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4180 - acc: 0.8260\n",
      "Epoch 110/200\n",
      "891/891 [==============================] - 0s 98us/step - loss: 0.4180 - acc: 0.8272\n",
      "Epoch 111/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4177 - acc: 0.8272\n",
      "Epoch 112/200\n",
      "891/891 [==============================] - 0s 104us/step - loss: 0.4175 - acc: 0.8249\n",
      "Epoch 113/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4178 - acc: 0.8283\n",
      "Epoch 114/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4171 - acc: 0.8249\n",
      "Epoch 115/200\n",
      "891/891 [==============================] - 0s 110us/step - loss: 0.4174 - acc: 0.8260\n",
      "Epoch 116/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4165 - acc: 0.8249\n",
      "Epoch 117/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4168 - acc: 0.8249\n",
      "Epoch 118/200\n",
      "891/891 [==============================] - 0s 113us/step - loss: 0.4164 - acc: 0.8227\n",
      "Epoch 119/200\n",
      "891/891 [==============================] - 0s 118us/step - loss: 0.4162 - acc: 0.8272\n",
      "Epoch 120/200\n",
      "891/891 [==============================] - 0s 115us/step - loss: 0.4158 - acc: 0.8227\n",
      "Epoch 121/200\n",
      "891/891 [==============================] - 0s 107us/step - loss: 0.4157 - acc: 0.8249\n",
      "Epoch 122/200\n",
      "891/891 [==============================] - 0s 122us/step - loss: 0.4151 - acc: 0.8238\n",
      "Epoch 123/200\n",
      "891/891 [==============================] - 0s 118us/step - loss: 0.4151 - acc: 0.8294\n",
      "Epoch 124/200\n",
      "891/891 [==============================] - 0s 117us/step - loss: 0.4150 - acc: 0.8260\n",
      "Epoch 125/200\n",
      "891/891 [==============================] - 0s 113us/step - loss: 0.4155 - acc: 0.8238\n",
      "Epoch 126/200\n",
      "891/891 [==============================] - 0s 110us/step - loss: 0.4145 - acc: 0.8260\n",
      "Epoch 127/200\n",
      "891/891 [==============================] - 0s 115us/step - loss: 0.4148 - acc: 0.8249\n",
      "Epoch 128/200\n",
      "891/891 [==============================] - 0s 118us/step - loss: 0.4144 - acc: 0.8272\n",
      "Epoch 129/200\n",
      "891/891 [==============================] - 0s 114us/step - loss: 0.4144 - acc: 0.8249\n",
      "Epoch 130/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4136 - acc: 0.8260\n",
      "Epoch 131/200\n",
      "891/891 [==============================] - 0s 108us/step - loss: 0.4142 - acc: 0.8249\n",
      "Epoch 132/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.4141 - acc: 0.8272\n",
      "Epoch 133/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4134 - acc: 0.8272\n",
      "Epoch 134/200\n",
      "891/891 [==============================] - 0s 123us/step - loss: 0.4134 - acc: 0.8249\n",
      "Epoch 135/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4126 - acc: 0.8227\n",
      "Epoch 136/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.4126 - acc: 0.8283\n",
      "Epoch 137/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4125 - acc: 0.8249\n",
      "Epoch 138/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4124 - acc: 0.8283\n",
      "Epoch 139/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.4122 - acc: 0.8260\n",
      "Epoch 140/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4129 - acc: 0.8227\n",
      "Epoch 141/200\n",
      "891/891 [==============================] - 0s 113us/step - loss: 0.4122 - acc: 0.8204\n",
      "Epoch 142/200\n",
      "891/891 [==============================] - 0s 114us/step - loss: 0.4120 - acc: 0.8249\n",
      "Epoch 143/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4119 - acc: 0.8215\n",
      "Epoch 144/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4114 - acc: 0.8238\n",
      "Epoch 145/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4118 - acc: 0.8227\n",
      "Epoch 146/200\n",
      "891/891 [==============================] - 0s 106us/step - loss: 0.4118 - acc: 0.8238\n",
      "Epoch 147/200\n",
      "891/891 [==============================] - 0s 104us/step - loss: 0.4114 - acc: 0.8227\n",
      "Epoch 148/200\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.4116 - acc: 0.8249\n",
      "Epoch 149/200\n",
      "891/891 [==============================] - 0s 106us/step - loss: 0.4115 - acc: 0.8249\n",
      "Epoch 150/200\n",
      "891/891 [==============================] - 0s 115us/step - loss: 0.4114 - acc: 0.8249\n",
      "Epoch 151/200\n",
      "891/891 [==============================] - 0s 118us/step - loss: 0.4110 - acc: 0.8227\n",
      "Epoch 152/200\n",
      "891/891 [==============================] - 0s 136us/step - loss: 0.4106 - acc: 0.8238\n",
      "Epoch 153/200\n",
      "891/891 [==============================] - 0s 129us/step - loss: 0.4106 - acc: 0.8260\n",
      "Epoch 154/200\n",
      "891/891 [==============================] - 0s 123us/step - loss: 0.4110 - acc: 0.8215\n",
      "Epoch 155/200\n",
      "891/891 [==============================] - 0s 134us/step - loss: 0.4104 - acc: 0.8227\n",
      "Epoch 156/200\n",
      "891/891 [==============================] - 0s 124us/step - loss: 0.4102 - acc: 0.8238\n",
      "Epoch 157/200\n",
      "891/891 [==============================] - 0s 125us/step - loss: 0.4099 - acc: 0.8227\n",
      "Epoch 158/200\n",
      "891/891 [==============================] - 0s 116us/step - loss: 0.4103 - acc: 0.8215\n",
      "Epoch 159/200\n",
      "891/891 [==============================] - 0s 124us/step - loss: 0.4099 - acc: 0.8227\n",
      "Epoch 160/200\n",
      "891/891 [==============================] - 0s 132us/step - loss: 0.4103 - acc: 0.8227\n",
      "Epoch 161/200\n",
      "891/891 [==============================] - 0s 128us/step - loss: 0.4104 - acc: 0.8227\n",
      "Epoch 162/200\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4097 - acc: 0.8227\n",
      "Epoch 163/200\n",
      "891/891 [==============================] - 0s 117us/step - loss: 0.4101 - acc: 0.8215\n",
      "Epoch 164/200\n",
      "891/891 [==============================] - 0s 113us/step - loss: 0.4095 - acc: 0.8227\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891/891 [==============================] - 0s 120us/step - loss: 0.4092 - acc: 0.8227\n",
      "Epoch 166/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.4092 - acc: 0.8227\n",
      "Epoch 167/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4092 - acc: 0.8227\n",
      "Epoch 168/200\n",
      "891/891 [==============================] - 0s 114us/step - loss: 0.4095 - acc: 0.8238\n",
      "Epoch 169/200\n",
      "891/891 [==============================] - 0s 122us/step - loss: 0.4096 - acc: 0.8227\n",
      "Epoch 170/200\n",
      "891/891 [==============================] - 0s 123us/step - loss: 0.4090 - acc: 0.8204\n",
      "Epoch 171/200\n",
      "891/891 [==============================] - 0s 122us/step - loss: 0.4087 - acc: 0.8215\n",
      "Epoch 172/200\n",
      "891/891 [==============================] - 0s 128us/step - loss: 0.4089 - acc: 0.8215\n",
      "Epoch 173/200\n",
      "891/891 [==============================] - 0s 131us/step - loss: 0.4089 - acc: 0.8238\n",
      "Epoch 174/200\n",
      "891/891 [==============================] - 0s 115us/step - loss: 0.4084 - acc: 0.8238\n",
      "Epoch 175/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4087 - acc: 0.8227\n",
      "Epoch 176/200\n",
      "891/891 [==============================] - 0s 109us/step - loss: 0.4084 - acc: 0.8204\n",
      "Epoch 177/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4086 - acc: 0.8215\n",
      "Epoch 178/200\n",
      "891/891 [==============================] - 0s 125us/step - loss: 0.4085 - acc: 0.8227\n",
      "Epoch 179/200\n",
      "891/891 [==============================] - 0s 128us/step - loss: 0.4088 - acc: 0.8249\n",
      "Epoch 180/200\n",
      "891/891 [==============================] - 0s 124us/step - loss: 0.4082 - acc: 0.8272\n",
      "Epoch 181/200\n",
      "891/891 [==============================] - 0s 120us/step - loss: 0.4082 - acc: 0.8238\n",
      "Epoch 182/200\n",
      "891/891 [==============================] - 0s 105us/step - loss: 0.4079 - acc: 0.8227\n",
      "Epoch 183/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4081 - acc: 0.8227\n",
      "Epoch 184/200\n",
      "891/891 [==============================] - 0s 99us/step - loss: 0.4082 - acc: 0.8260\n",
      "Epoch 185/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4076 - acc: 0.8249\n",
      "Epoch 186/200\n",
      "891/891 [==============================] - 0s 97us/step - loss: 0.4078 - acc: 0.8227\n",
      "Epoch 187/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4080 - acc: 0.8238\n",
      "Epoch 188/200\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4077 - acc: 0.8238\n",
      "Epoch 189/200\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4071 - acc: 0.8227\n",
      "Epoch 190/200\n",
      "891/891 [==============================] - 0s 106us/step - loss: 0.4081 - acc: 0.8227\n",
      "Epoch 191/200\n",
      "891/891 [==============================] - 0s 160us/step - loss: 0.4079 - acc: 0.8238\n",
      "Epoch 192/200\n",
      "891/891 [==============================] - 0s 146us/step - loss: 0.4078 - acc: 0.8249\n",
      "Epoch 193/200\n",
      "891/891 [==============================] - 0s 141us/step - loss: 0.4074 - acc: 0.8249\n",
      "Epoch 194/200\n",
      "891/891 [==============================] - 0s 226us/step - loss: 0.4076 - acc: 0.8238\n",
      "Epoch 195/200\n",
      "891/891 [==============================] - 0s 133us/step - loss: 0.4077 - acc: 0.8238\n",
      "Epoch 196/200\n",
      "891/891 [==============================] - 0s 130us/step - loss: 0.4077 - acc: 0.8227\n",
      "Epoch 197/200\n",
      "891/891 [==============================] - 0s 130us/step - loss: 0.4073 - acc: 0.8260\n",
      "Epoch 198/200\n",
      "891/891 [==============================] - 0s 141us/step - loss: 0.4071 - acc: 0.8249\n",
      "Epoch 199/200\n",
      "891/891 [==============================] - 0s 121us/step - loss: 0.4070 - acc: 0.8215\n",
      "Epoch 200/200\n",
      "891/891 [==============================] - 0s 127us/step - loss: 0.4069 - acc: 0.8249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3dc48b1d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "classifier_dl  = Sequential()\n",
    "classifier_dl.add(Dense(output_dim = 4,input_dim = 6, activation = 'relu',kernel_initializer = 'uniform'))\n",
    "classifier_dl.add(Dense(output_dim = 3, activation = 'relu',kernel_initializer = 'uniform'))\n",
    "classifier_dl.add(Dense(output_dim = 1, activation = 'sigmoid',kernel_initializer = 'uniform'))\n",
    "# Compiling the ANN\n",
    "classifier_dl.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier_dl.fit(X_train_scaled, y, batch_size = 10, epochs = 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier_dl.predict(X_test_scaled)\n",
    "y_pre_dl = (y_pred > 0.5)\n",
    "y_pred_dl= y_pre_dl.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"submit_avg.csv\", submit, dddelimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_rf = np.column_stack((pid,y_pred_rf))\n",
    "np.savetxt(\"submit_rf.csv\", submit, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_ksvm = np.column_stack((pid,y_pred_ksvm))\n",
    "np.savetxt(\"submit_ksvm.csv\", submit, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_ksvm = np.column_stack((pid,y_pred_knn))\n",
    "np.savetxt(\"submit_knn.csv\", submit, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_ksvm = np.column_stack((pid,y_pred_nb))\n",
    "np.savetxt(\"submit_nb.csv\", submit, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_ksvm = np.column_stack((pid,y_pred_lr))\n",
    "np.savetxt(\"submit_lr.csv\", submit, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
